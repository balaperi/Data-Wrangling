{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrangling WeRateDogs twitter data was quite challenging and time consuming.I tried to apply most of the concepts that have been taught in the lectures.\n",
    "\n",
    "In the data gathering stage,Twitter archive data was readily available for download.For the image predictions data,the tsv file in the given url is downloaded using the request method.The tsv is then read to csv with the separator parmater to \\t.\n",
    "The gathering of tweet count data was quite challenging as the file was in json format.The file is opened,then scanned for tweet_id,retweets,favorites counts and appended in a list.Followed by creating a dataframe using the list and columns names.\n",
    "\n",
    "In assessing the data,there are 3 dataframes one for twitter archive data,image predictions and tweet count data.\n",
    "Twitter archive dataframe is the largest in terms of number of records and columns among other 2 datasets.The data is checked for duplicates,nulls,missing values using pandas head,info functions.Some visual analysis,were also made,for columns having missing values,null values.Programmatic analysis were also made by summarizing,filtering the data and evaluating the features.Table reshaping was also done for converting few columns into values .Other 2 datasets were merged with the twitter archive dataset.\n",
    "\n",
    "The final step in data wrangling,is data cleaning.For each operation,a define,code and test format was used,wherein each operation is defined,then made programmatic code changes and tested the code on the datasets for the desired results.Some operations include,dropping columns with missing values,renaming columns,extracting the part of data from the column values for easy understanding,data removal was also done to exclude retweets.\n",
    "\n",
    "With the clean twitter master dataset,some analysis and few visualisations were also done.\n",
    "Throughout this project ,I got to work from data gathering ,identifying data quality and tidiness issues and then cleaning up the data for analysis and visualisations.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
